# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PHASE 2 - Google Custom Search API (VERSION LOCALE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Cherche LinkedIn & Twitter pour les profils manquants via Google Search API.
# Utilise un Rate Limiter intÃ©grÃ© pour respecter le quota (100 requÃªtes/minute).
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# ğŸ“¥ IMPORTS STANDARDS
import json
import re
import time
import os
import threading
from googleapiclient.discovery import build
from tqdm import tqdm # Utilisation de tqdm standard
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  CONFIGURATION GOOGLE API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INPUT_FILENAME = 'github_profiles_missing_links.json'

print("="*70)
print("CONFIGURATION GOOGLE CUSTOM SEARCH API")
print("="*70)
print("\nVotre quota : 10,000 requÃªtes/jour | Limite : 100 requÃªtes/minute\n")

# Utilisation de input() pour le terminal local
api_key = input("Entrez votre API Key Google: ")
search_engine_id = input("Entrez votre Search Engine ID (CX): ")

API_CONFIG = {
    'api_key': api_key,
    'cx': search_engine_id
}

print("\nConfiguration terminÃ©e.")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  RATE LIMITER (90 requÃªtes/minute max pour sÃ©curitÃ©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RateLimiter:
    """Limite Ã  90 requÃªtes/minute pour sÃ©curitÃ©"""
    def __init__(self, max_calls=90, time_window=60):
        self.max_calls = max_calls
        self.time_window = time_window
        self.calls = []
        self.lock = threading.Lock()

    def wait_if_needed(self):
        with self.lock:
            now = time.time()
            # Supprimer les appels hors fenÃªtre
            self.calls = [t for t in self.calls if now - t < self.time_window]

            if len(self.calls) >= self.max_calls:
                # Attendre que le plus vieux appel sorte de la fenÃªtre
                sleep_time = self.time_window - (now - self.calls[0]) + 1
                print(f"\nRate limit : attente {sleep_time:.0f}s...")
                time.sleep(sleep_time)
                # Remise Ã  zÃ©ro pour la nouvelle fenÃªtre
                self.calls = [t for t in self.calls if time.time() - t < self.time_window] 

            self.calls.append(time.time()) # Utiliser le temps actuel aprÃ¨s l'attente

rate_limiter = RateLimiter(max_calls=90, time_window=60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  CHARGEMENT DES DONNÃ‰ES LOCALES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if not os.path.exists(INPUT_FILENAME):
    print(f"\nERREUR: Fichier introuvable! Placez '{INPUT_FILENAME}' dans le mÃªme dossier que ce script.")
    exit(1)

# Charger les profils sans liens
with open(INPUT_FILENAME, 'r', encoding='utf-8') as f:
    data = json.load(f)

# Extraire la liste des profils
if isinstance(data, dict) and 'profiles' in data:
    profiles = data['profiles']
else:
    profiles = data

print(f"\n{len(profiles):,} profils Ã  traiter chargÃ©s depuis {INPUT_FILENAME}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  FONCTIONS DE RECHERCHE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_linkedin_from_results(items):
    """Extrait LinkedIn depuis rÃ©sultats Google"""
    if not items:
        return None

    patterns = [
        r'linkedin\.com/in/([a-zA-Z0-9-]+)',
        r'www\.linkedin\.com/in/([a-zA-Z0-9-]+)'
    ]

    for item in items:
        link = item.get('link', '')

        # VÃ©rifier le lien direct
        for pattern in patterns:
            match = re.search(pattern, link, re.IGNORECASE)
            if match:
                username = match.group(1)
                # Filtrer les URLs gÃ©nÃ©riques
                if username.lower() not in ['linkedin', 'in', 'company', 'pub', 'posts', 'jobs']:
                    return f"https://linkedin.com/in/{username}"

    # Si pas trouvÃ© dans les liens, chercher dans les snippets
    for item in items:
        snippet = item.get('snippet', '')
        for pattern in patterns:
            match = re.search(pattern, snippet, re.IGNORECASE)
            if match:
                username = match.group(1)
                if username.lower() not in ['linkedin', 'in', 'company', 'pub', 'posts', 'jobs']:
                    return f"https://linkedin.com/in/{username}"

    return None

def extract_twitter_from_results(items):
    """Extrait Twitter depuis rÃ©sultats Google"""
    if not items:
        return None

    patterns = [
        r'twitter\.com/([a-zA-Z0-9_]+)',
        r'x\.com/([a-zA-Z0-9_]+)'
    ]

    for item in items:
        link = item.get('link', '')

        # VÃ©rifier le lien direct
        for pattern in patterns:
            match = re.search(pattern, link, re.IGNORECASE)
            if match:
                username = match.group(1)
                # Filtrer les URLs gÃ©nÃ©riques
                if username.lower() not in ['twitter', 'x', 'intent', 'share', 'search', 'i', 'explore', 'home']:
                    return f"https://twitter.com/{username}"

    # Si pas trouvÃ© dans les liens, chercher dans les snippets
    for item in items:
        snippet = item.get('snippet', '')
        for pattern in patterns:
            match = re.search(pattern, snippet, re.IGNORECASE)
            if match:
                username = match.group(1)
                if username.lower() not in ['twitter', 'x', 'intent', 'share', 'search', 'i', 'explore', 'home']:
                    return f"https://twitter.com/{username}"

    return None

def google_search(query, retries=3):
    """Effectue une recherche Google avec rate limiting"""
    for attempt in range(retries):
        try:
            # Attendre si nÃ©cessaire (rate limiting)
            rate_limiter.wait_if_needed()

            # La crÃ©ation du service peut rester Ã  l'extÃ©rieur si la clÃ© est statique, mais elle est ici pour la robustesse.
            service = build("customsearch", "v1", developerKey=API_CONFIG['api_key'])
            result = service.cse().list(
                q=query,
                cx=API_CONFIG['cx'],
                num=10
            ).execute()
            
            return result.get('items', [])

        except Exception as e:
            error_str = str(e).lower()

            # Quota journalier dÃ©passÃ©
            if 'quota' in error_str and 'day' in error_str:
                print(f"\nERREUR: QUOTA JOURNALIER DÃ‰PASSÃ‰ (10,000 requÃªtes)")
                print(f" Attendez demain ou utilisez une autre clÃ© API.")
                return None

            # Rate limit par minute
            if 'quota' in error_str or 'rate' in error_str or '429' in error_str:
                wait_time = (attempt + 1) * 10
                print(f"\nRate limit dÃ©tectÃ©, attente {wait_time}s...")
                time.sleep(wait_time)
                continue

            # Autre erreur
            if attempt == retries - 1:
                print(f"\nErreur recherche: {str(e)[:100]}")
                return None

            time.sleep(2)

    return None

def search_profile_social_links(profile):
    """Cherche LinkedIn et Twitter pour un profil"""
    result = {
        'username': profile['username'],
        'name': profile.get('name'),
        'github_url': profile.get('github_url'),
        'linkedin': None,
        'twitter': None,
        'source': []
    }

    # Construire les requÃªtes
    name = profile.get('name') or profile['username']
    username = profile['username']

    # Recherche LinkedIn
    query_linkedin = f'"{name}" OR "{username}" site:linkedin.com/in Morocco developer'
    items_linkedin = google_search(query_linkedin)

    if items_linkedin:
        result['linkedin'] = extract_linkedin_from_results(items_linkedin)
        if result['linkedin']:
            result['source'].append('google_linkedin')

    # Recherche Twitter
    query_twitter = f'"{name}" OR "{username}" (site:twitter.com OR site:x.com) Morocco developer'
    items_twitter = google_search(query_twitter)

    if items_twitter:
        result['twitter'] = extract_twitter_from_results(items_twitter)
        if result['twitter']:
            result['source'].append('google_twitter')

    return result

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  TRAITEMENT (MODE SÃ‰QUENTIEL / RATE LIMIT GÃ‰RÃ‰)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print(f"\nTraitement de {len(profiles):,} profils...")
print(f"Mode : SÃ©quentiel (limite de 90 req/min)")

start_time = time.time()
results = []
errors = []

# Compteurs
linkedin_found = 0
twitter_found = 0
both_found = 0
requests_made = 0

# Traitement sÃ©quentiel avec barre de progression
for i, profile in enumerate(tqdm(profiles, desc="Recherche Google")):
    try:
        result = search_profile_social_links(profile)
        results.append(result)

        # Mise Ã  jour des compteurs
        requests_made += 2  # 2 requÃªtes par profil
        if result['linkedin']:
            linkedin_found += 1
        if result['twitter']:
            twitter_found += 1
        if result['linkedin'] and result['twitter']:
            both_found += 1

    except Exception as e:
        errors.append({'username': profile.get('username'), 'error': str(e)})

elapsed_time = time.time() - start_time

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  STATISTIQUES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

one_found = sum(1 for r in results if (r['linkedin'] or r['twitter']) and not (r['linkedin'] and r['twitter']))
none_found = sum(1 for r in results if not r['linkedin'] and not r['twitter'])

print("\n" + "="*70)
print("RESULTATS PHASE 2 - GOOGLE SEARCH")
print("="*70)
print(f"Temps de traitement : {elapsed_time/60:.1f} minutes ({elapsed_time/3600:.1f}h)")
print(f"RequÃªtes effectuÃ©es : {requests_made:,}")
print(f"Vitesse moyenne Â  Â  : {requests_made/(elapsed_time/60):.1f} req/min")
print()
print(f"Total traitÃ© Â  Â  Â : {len(results):,} profils")
print(f"LinkedIn trouvÃ© Â  : {linkedin_found:,} ({linkedin_found/len(results)*100:.1f}%)")
print(f"Twitter trouvÃ© Â  Â : {twitter_found:,} ({twitter_found/len(results)*100:.1f}%)")
print(f"Les DEUX trouvÃ©s Â : {both_found:,} ({both_found/len(results)*100:.1f}%)")
print(f"Un seul trouvÃ© Â  Â : {one_found:,} ({one_found/len(results)*100:.1f}%)")
print(f"Rien trouvÃ© Â  Â  Â  : {none_found:,} ({none_found/len(results)*100:.1f}%)")
if errors:
    print(f"Erreurs Â  Â  Â  Â  Â : {len(errors)}")
print("="*70)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SAUVEGARDE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nSauvegarde des rÃ©sultats...")

# RÃ©sultats complets Phase 2
output = {
    'metadata': {
        'phase': 2,
        'method': 'Google Custom Search API',
        'total_profiles': len(results),
        'requests_made': requests_made,
        'linkedin_found': linkedin_found,
        'twitter_found': twitter_found,
        'both_found': both_found,
        'processing_time_minutes': round(elapsed_time/60, 2),
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    },
    'profiles': results
}

with open('phase2_google_results.json', 'w', encoding='utf-8') as f:
    json.dump(output, f, indent=2, ensure_ascii=False)

# Profils avec liens trouvÃ©s
found = [r for r in results if r['linkedin'] or r['twitter']]
with open('phase2_links_found.json', 'w', encoding='utf-8') as f:
    json.dump(found, f, indent=2, ensure_ascii=False)

# Profils encore sans liens
still_missing = [r for r in results if not r['linkedin'] and not r['twitter']]
missing_output = {
    'count': len(still_missing),
    'profiles': still_missing
}
with open('phase3_still_missing.json', 'w', encoding='utf-8') as f:
    json.dump(missing_output, f, indent=2, ensure_ascii=False)


import csv
with open('phase2_google_results.csv', 'w', newline='', encoding='utf-8-sig') as f:
    writer = csv.DictWriter(f, fieldnames=['username', 'name', 'github_url', 'linkedin', 'twitter', 'source'])
    writer.writeheader()
    for r in results:
        writer.writerow({
            'username': r['username'],
            'name': r['name'] or '',
            'github_url': r['github_url'],
            'linkedin': r['linkedin'] or '',
            'twitter': r['twitter'] or '',
            'source': ', '.join(r['source'])
        })

print("\nFichiers sauvegardÃ©s:")
print(" Â  phase2_google_results.json")
print(" Â  phase2_links_found.json")
print(" Â  phase3_still_missing.json")
print(" Â  phase2_google_results.csv")



print(f"\nTWITTER:")
print(f" Â  Phase 1 : {phase1_twitter:,}")
print(f" Â  Phase 2 : +{phase2_twitter:,}")
print(f" Â  TOTAL Â  : {total_twitter:,} ({total_twitter/total_profiles*100:.1f}%)")

print(f"\nCOUVERTURE GLOBALE:")
print(f" Â  Profils avec liens : {total_with_links:,} / {total_profiles:,}")
print(f" Â  Taux de couverture : {total_with_links/total_profiles*100:.1f}%")
print(f" Â  Profils restants Â  : {len(still_missing):,}")

total_links_all = total_linkedin + total_twitter
print(f"\nTOTAL DE LIENS TROUVÃ‰S : {total_links_all:,}")
print(f" Â  Sur {total_profiles * 2:,} liens possibles")
print(f" Â  Taux de succÃ¨s : {total_links_all/(total_profiles*2)*100:.1f}%")

print("\n" + "="*70)
print("PHASE 2 TERMINÃ‰E !")
print("="*70)

# Rapport final
report = f"""
RAPPORT PHASE 2 - GOOGLE CUSTOM SEARCH API
==========================================
Date : {time.strftime('%Y-%m-%d %H:%M:%S')}
DurÃ©e : {elapsed_time/60:.1f} minutes ({elapsed_time/3600:.1f} heures)

TRAITEMENT:
- Profils traitÃ©s : {len(results):,}
- RequÃªtes effectuÃ©es : {requests_made:,}
- Vitesse moyenne : {requests_made/(elapsed_time/60):.1f} requÃªtes/min

RÃ‰SULTATS PHASE 2:
- LinkedIn trouvÃ©s : {linkedin_found:,} ({linkedin_found/len(results)*100:.1f}%)
- Twitter trouvÃ©s : {twitter_found:,} ({twitter_found/len(results)*100:.1f}%)
- Les deux trouvÃ©s : {both_found:,} ({both_found/len(results)*100:.1f}%)
- Rien trouvÃ© : {none_found:,} ({none_found/len(results)*100:.1f}%)

RÃ‰SULTATS GLOBAUX (Phase 1 + 2):
- LinkedIn total : {total_linkedin:,} / {total_profiles:,} ({total_linkedin/total_profiles*100:.1f}%)
- Twitter total : {total_twitter:,} / {total_profiles:,} ({total_twitter/total_profiles*100:.1f}%)
- Couverture : {total_with_links:,} / {total_profiles:,} ({total_with_links/total_profiles*100:.1f}%)
- Profils restants sans liens : {len(still_missing):,}

FICHIERS GÃ‰NÃ‰RÃ‰S:
- phase2_google_results.json (tous les rÃ©sultats)
- phase2_links_found.json (seulement avec liens)
- phase3_still_missing.json (pour phase 3)
- phase2_google_results.csv (format Excel)
"""

with open('phase2_report.txt', 'w', encoding='utf-8') as f:
    f.write(report)

print("\nRapport sauvegardÃ© : phase2_report.txt")
print(f"\nFÃ©licitations ! Vous avez maintenant ~{total_with_links/total_profiles*100:.0f}% de couverture !")